{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Start] R2DE2 WS2 - Data Cleansing with Spark",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phawit9/r2de-colab/blob/main/%5BStart%5D_R2DE2_WS2_Data_Cleansing_with_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFKfSj8ZyWyd"
      },
      "source": [
        "# สำคัญ: ถ้าคุณเปิดไฟล์นี้จากลิงค์ในบทเรียนบนเว็บไซต์\n",
        "\n",
        "# กรุณากด File > Save a Copy in Drive เพื่อก็อปปี้ไฟล์นี้ไปก่อน\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SQc75fHqX85"
      },
      "source": [
        "# Workshop 2: Data Cleansing with Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnxzvnOmsN-X"
      },
      "source": [
        "ภาพรวมของคอร์สนี้\n",
        "\n",
        "![alt text](https://cdn-std.droplr.net/files/acc_513973/z7Gqhs)\n",
        "\n",
        "Workshop 2 นี้เราจะทำอะไรกันบ้าง\n",
        "\n",
        "![alt text](https://cdn-std.droplr.net/files/acc_513973/SCN8wh)\n",
        "\n",
        "![](https://file.designil.com/4Cc26a+)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H86b_xW2tAo9"
      },
      "source": [
        "## ลิงค์ดีมีประโยชน์\n",
        "\n",
        "### 1. หน้าเว็บไซต์รวมคำสั่งทั้งหมดของ PySpark\n",
        "\n",
        "หน้าเว็บไซต์นี้อัพเดทข้อมูลโดยทีมงาน Apache Spark พร้อมวิธีใช้ และคำอธิบายค่าต่าง ๆ ที่คำสั่งนั้นสามารถรับได้ สามารถค้นหาคำสั่งที่ต้องการได้ด้วย\n",
        "\n",
        "https://spark.apache.org/docs/latest/api/python/reference/index.html\n",
        "\n",
        "### 2. [ภาษาอังกฤษ] Spark Cheatsheet by DataCamp \n",
        "\n",
        "รวมคำสั่ง Spark\n",
        "\n",
        "**RDD:**\n",
        "https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python \n",
        "\n",
        "**DataFrame:**\n",
        "https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet\n",
        "\n",
        "### 3. [ภาษาไทย] Spark Cheatsheet by DataTH\n",
        "\n",
        "รวมคำสั่ง Spark พร้อมตัวอย่างโค้ด และคำอธิบายภาษาไทย\n",
        "\n",
        "https://blog.datath.com/cheatsheet-pyspark/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8fv4Fl2yqXJ"
      },
      "source": [
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l1rCEL01qMW"
      },
      "source": [
        "# มาเริ่ม Workshop 2 กันเลย\n",
        "\n",
        "![](https://file.designil.com/VrNxQe+)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ06tCiTn7z9"
      },
      "source": [
        "# Step 1) ติดตั้ง Spark และ PySpark\n",
        "\n",
        "Google Colab เป็นเครื่องมือสำหรับรันคำสั่ง Python และ Bash บนคอมพิวเตอร์จำลองที่ Google เตรียมไว้ให้เรา\n",
        "\n",
        "คอมพิวเตอร์จำลองนี้เรียกว่า Virtual Machine (VM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SV-YTNRpMKy"
      },
      "source": [
        "!apt-get update                                                                          # อัพเดท Package ทั้งหมดใน VM ตัวนี้\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null                                  # ติดตั้ง Java Development Kit (จำเป็นสำหรับการติดตั้ง Spark)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz # ติดตั้ง Spark 3.1.2\n",
        "!tar xzvf spark-3.1.2-bin-hadoop2.7.tgz                                                  # Unzip ไฟล์ Spark 3.1.2\n",
        "!pip install -q findspark==1.3.0                                                         # ติดตั้ง Package Python สำหรับเชื่อมต่อกับ Spark "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dwpn8wv9lgO"
      },
      "source": [
        "# Set enviroment variable ให้ Python รู้จัก Spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHNIVIS8ygwc"
      },
      "source": [
        "# ติดตั้ง PySpark ลงใน Python\n",
        "!pip install pyspark==3.1.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aX_fiEkrkVo"
      },
      "source": [
        "## ใช้งาน Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASZqLPXDPJJB"
      },
      "source": [
        "# Server ของ Google Colab มีกี่ Core\n",
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOC1AgcQwRz1"
      },
      "source": [
        "ใช้ `local[*]` เพื่อเปิดการใช้งานการประมวลผลแบบ multi-core (Spark จะใช้ CPU ทุก core ที่อนุญาตให้ใช้งานในเครื่อง)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6-PYi44nH2e"
      },
      "source": [
        "# สร้าง Spark Session เพิ้อใช้งาน Spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Fuuh_8ocdw"
      },
      "source": [
        "# ดูเวอร์ชั่น Python\n",
        "import sys\n",
        "sys.version_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obpcIPP4oOvk"
      },
      "source": [
        "# ดูเวอร์ชั่น Spark\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t8xQWVEi7tU"
      },
      "source": [
        "## Load Workshop 2 Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48EFzlQd-FET"
      },
      "source": [
        "คำอธิบายคำสั่ง:\n",
        "\n",
        "wget = คำสั่งในการดาวน์โหลดไฟล์\n",
        "\n",
        "wget -O = ตั้งชื่อไฟล์"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a0K_ljDi-r5"
      },
      "source": [
        "# Download Data File\n",
        "!wget -O data.zip https://file.designil.com/zdOfUE+\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pn2zW6xqrtY"
      },
      "source": [
        "### Load data ใส่ Spark\n",
        "\n",
        "ใช้คำสั่ง `spark.read.csv` เพื่ออ่านข้อมูลจากไฟล์ CSV\n",
        "\n",
        "Arguments:\n",
        "\n",
        "Header = True << บอกให้ Spark รู้ว่าบรรทัดแรกในไฟล์ CSV เป็น Header\n",
        "\n",
        "Inferschema = True << บอกให้ Spark พยายามเดาว่าแต่ละ column มี type เป็นอะไร [ ถ้าตั้งเป็น False, ทุก column จะถูกอ่านเป็น string (ตัวหนังสือ) ]\n",
        "\n",
        "Reference: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.csv.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ho4MvryoY9u"
      },
      "source": [
        "dt = spark.read.csv('/content/ws2_data.csv', header = True, inferSchema = True, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXRH_sPL17sL"
      },
      "source": [
        "---\n",
        "\n",
        "![](https://file.designil.com/2c6qGS+)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKBjHvpzTM6-"
      },
      "source": [
        "# Step 2) Data Profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruTiAh69nkWT"
      },
      "source": [
        "Data Profiling เป็นการทำความเข้าใจข้อมูลเบื้องต้น เพื่อที่เราจะได้รู้ว่าข้อมูลนี้มีคอลัมน์ไหนบ้าง ค่าโดยรวมเป็นอย่างไรบ้าง ฯลฯ เพื่อให้เราตัดสินใจได้ต่อว่าจะเช็คที่จุดไหนต่อไป\n",
        "\n",
        "ตัวอย่าง: max, min, average, sum, มี missing value มั้ย ฯลฯ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpb7PcpSrOWe"
      },
      "source": [
        "# ดูว่ามีคอลัมน์อะไรบ้าง\n",
        "dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpp8rs5mrcjo"
      },
      "source": [
        "> Columns:\n",
        "1. timestamp\n",
        "2. user_id\n",
        "3. book_id\n",
        "4. country\n",
        "5. price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rEzUAWCS3Gd"
      },
      "source": [
        "# ดูข้อมูล\n",
        "dt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOgygtSZUenr"
      },
      "source": [
        "# ดูข้อมูล 100 แถวแรก\n",
        "dt.show(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hfU4p2sCuef"
      },
      "source": [
        "# ดูประเภทข้อมูลแต่ละคอลัมน์\n",
        "dt.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er9ltLyoCJxo"
      },
      "source": [
        "# อีกคำสั่งในการดูข้อมูลแต่ละคอลัมน์ (Schema)\n",
        "dt.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu34YbPnnCuL"
      },
      "source": [
        "nullable คือ ค่าสามารถเป็น null ได้\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brQdwlRpCVSi"
      },
      "source": [
        "# นับจำนวนแถวและ column\n",
        "print((dt.count(), len(dt.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ9V4iQKUi2i"
      },
      "source": [
        "# สรุปข้อมูลสถิติ\n",
        "dt.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ7xtdKgVvKO"
      },
      "source": [
        "# อีกคำสั่งในการสรุปข้อมูลสถิติ\n",
        "# ReferenceL: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.summary.html\n",
        "\n",
        "dt.summary().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBmjzCqjM_MR"
      },
      "source": [
        "# สรุปข้อมูลสถิติเฉพาะ column ที่ระบุ\n",
        "dt.select(\"price\").describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7gUknDUB8Wj"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "คอลัมน์ไหนมี Missing Value บ้าง? และแสดงข้อมูลแถวที่มี Missing Value ให้ดูหน่อย\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLnRRd2MrvAl"
      },
      "source": [
        "# Answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP9xaB8k2BZs"
      },
      "source": [
        "---\n",
        "\n",
        "![](https://file.designil.com/D9H1d3+)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe1VrTpETgar"
      },
      "source": [
        "# Step 3) EDA - Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRmig4MaQm2s"
      },
      "source": [
        "## Non-Graphical EDA\n",
        "\n",
        "เราสามารถใช้คำสั่ง Spark ในการค้นหาข้อมูลที่ต้องการได้"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGkcJ3kDQmJ5"
      },
      "source": [
        "# ข้อมูลที่เป็นตัวเลข\n",
        "dt.where(dt.price >= 1).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccWQBqMXxd2O"
      },
      "source": [
        "# ข้อมูลที่เป็นตัวหนังสือ\n",
        "dt.where(dt.country == 'Canada').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOnfZz27XhjX"
      },
      "source": [
        "### Exercise 2: \n",
        "1. การซื้อทั้งหมดที่เกิดขึ้นในเดือนเมษายน มีกี่แถว\n",
        "2. การซื้อทั้งหมดที่เกิดขึ้นในเดือนสิงหาคม มีกี่แถว"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgPm8dsraxNx"
      },
      "source": [
        "# Answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN3m9gpHkS5H"
      },
      "source": [
        "## Graphical EDA\n",
        "\n",
        "\n",
        "Spark ไม่ได้ถูกพัฒนามาเพื่องาน plot ข้อมูล เพราะฉะนั้นเราจะใช้ package `seaborn` `matplotlib` และ `pandas` ในการ plot ข้อมูลแทน"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWu_zxdONk0o"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "861ui21dNy-N"
      },
      "source": [
        "# แปลง Spark Dataframe เป็น Pandas Dataframe - ใช้เวลาประมาณ 6 วินาที\n",
        "dt_pd = dt.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpF73lu_Rn-_"
      },
      "source": [
        "# ดูตัวอย่างข้อมูล\n",
        "dt_pd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw91uylC7B_x"
      },
      "source": [
        "# Boxplot - แสดงการกระจายตัวของข้อมูลตัวเลข\n",
        "sns.boxplot(x = dt_pd['book_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYbSz2HuO2sr"
      },
      "source": [
        "# Histogram - แสดงการกระจายตัวของข้อมูลตัวเลข\n",
        "# bins = จำนวน bar ที่ต้องการแสดง\n",
        "sns.histplot(dt_pd['price'], bins=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-xTqiuGERU_"
      },
      "source": [
        "### Exercise 3: \n",
        "book_id เพิ่มขึ้นตามราคาหรือเปล่า?\n",
        "\n",
        "ลองสร้าง Plot เพื่อดูความสัมพันธ์ระหว่าง book_id กับ price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1N-03d9O83E"
      },
      "source": [
        "# Answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVliQZLC0LuP"
      },
      "source": [
        "#### Bonus: สร้าง interactive chart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bb0F-9zTI1X"
      },
      "source": [
        "# Plotly - interactive chart\n",
        "import plotly.express as px\n",
        "fig = px.scatter(dt_pd, 'book_id', 'price')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6InlOOw32IDS"
      },
      "source": [
        "---\n",
        "\n",
        "![](https://file.designil.com/Huzkx0+)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbAnv3_Rwj5l"
      },
      "source": [
        "# Step 4) Data Cleansing with Spark\n",
        "\n",
        "มาทำความสะอาดข้อมูลด้วย Spark กันเถอะ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXLHVsbwk08q"
      },
      "source": [
        "### แปลง Data Type\n",
        "\n",
        "ปัญหาที่เจอบ่อยที่สุดแบบหนึ่งในข้อมูล คือ **Data Type ไม่ตรงกับที่เราต้องการ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb7BB-e3k4g-"
      },
      "source": [
        "# Show top 5 rows\n",
        "dt.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7gFtSr0Fbne"
      },
      "source": [
        "# Show Schema\n",
        "dt.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNVwy-FkHDrT"
      },
      "source": [
        "จะเห็นว่า `Timestamp` ถูกอ่านเป็นข้อมูลตัวหนังสือ (String) แต่เราอยากให้มันเป็นข้อมูลวันที่และเวลา (date time) จะทำยังไงดี?\n",
        "\n",
        "ก่อนอื่น เราต้องมาดูก่อนว่าคอลัมน์ Timestamp แสดงเลขวันที่ก่อน หรือเลขเดือนก่อน (DD/MM/YYYY หรือ MM/DD/YYYY)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu6po7nVGhj6"
      },
      "source": [
        "dt.select(\"timestamp\").show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s53J7rfXIvXH"
      },
      "source": [
        "เราจะมาใช้ฟังก์ชั่น to_timestamp ซึ่งอยู่ใน pyspark.sql.functions กัน\n",
        "\n",
        "Reference: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.to_timestamp.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnerFBErFelC"
      },
      "source": [
        "# แปลง string เป็น datetime\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "dt_clean = dt.withColumn(\"timestamp\",\n",
        "                        f.to_timestamp(dt.timestamp, 'yyyy-MM-dd HH:mm:ss')\n",
        "                        )\n",
        "dt_clean.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq7sQ-h6T6cp"
      },
      "source": [
        "dt_clean.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMAQ-mXsNf1z"
      },
      "source": [
        "## BONUS: ตัวอย่างการใช้ประโยชน์จากข้อมูล Datetime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMiWFpDtJFdM"
      },
      "source": [
        "# นับยอด transaction ช่วงครึ่งเดือนแรก ของเดือนมิถุนายน\n",
        "dt_clean.where( (f.dayofmonth(dt_clean.timestamp) <= 15) & ( f.month(dt_clean.timestamp) == 6 ) ).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlewC3HjbMty"
      },
      "source": [
        "## Anomalies Check\n",
        "\n",
        "ใช้ Spark ตามหาสิ่งที่ผิดปกติในข้อมูล"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQmzIi6c2Ugp"
      },
      "source": [
        "### ความผิดปกติ 1) Syntactical Anomalies\n",
        "**Lexical errors** เช่น สะกดผิด"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKKS4l0-RUJE"
      },
      "source": [
        "#### Exercise 4\n",
        "\n",
        "หาชื่อประเทศที่สะกดผิด แล้วแก้ชื่อที่สะกดผิดให้ถูก"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htfmeHuZ2VPo"
      },
      "source": [
        "# ใน Data set ชุดนี้ มีข้อมูลจากกี่ประเทศ\n",
        "dt_clean.select(\"...\").distinct().count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Ei25HVQcC1"
      },
      "source": [
        "# แทนที่ ... ด้วยจำนวนประเทศ เพื่อดูรายชื่อประเทศทั้งหมด\n",
        "# sort = ทำให้ข้อมูลเรียงตามตัวอักษร อ่านง่ายขึ้น\n",
        "# show() ถ้าไม่ใส่ตัวเลขจะขึ้นมาแค่ 20 อัน และใส่ False เพื่อให้แสดงข้อมูลในคอลัมน์แบบเต็ม ๆ (หากไม่ใส่ คอลัมน์ที่ยาวจะถูกตัดตัวหนังสือ)\n",
        "dt_clean.select(\"Country\").distinct().sort(\"Country\").show( ..., False )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE0Sxc2uRzNv"
      },
      "source": [
        "มาดูกันว่าประเทศที่ชื่อผิด มีข้อมูลหน้าตาเป็นอย่างไร"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSGUoi-eUl5U"
      },
      "source": [
        "# เปลี่ยน ... เป็นชื่อประเทศที่คุณคิดว่าผิด\n",
        "dt_clean.where(dt_clean['Country'] == '...').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx5pu23dR7kV"
      },
      "source": [
        "ได้เวลาลองเปลี่ยนชื่อประเทศให้สะกดถูกต้อง"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03YDBKivjIfh"
      },
      "source": [
        "# เปลี่ยน ... เป็นชื่อประเทศที่คุณคิดว่าผิด และ ...2 เป็นชื่อประเทศที่ถูกต้อง\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "dt_clean_country = dt_clean.withColumn(\"CountryUpdate\", when(dt_clean['Country'] == '...', '...2').otherwise(dt_clean['Country']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHp2xBfIknNx"
      },
      "source": [
        "# ตรวจสอบข้อมูลที่แก้ไขแล้ว\n",
        "dt_clean_country.select(\"...\").distinct().sort(\"...\").show(..., False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkNf0x8oT6HB"
      },
      "source": [
        "# ดูหน้าตาข้อมูลตอนนี้\n",
        "dt_clean_country.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXspKbmMlDEA"
      },
      "source": [
        "# เอาคอลัมน์ CountryUpdate ไปแทนที่คอลัมน์ Country\n",
        "dt_clean = dt_clean_country.drop(\"Country\").withColumnRenamed('CountryUpdate', 'Country')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UP2l5g-VkFJ"
      },
      "source": [
        "# ดูหน้าตาข้อมูล\n",
        "...show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2T8jBXEUAEz"
      },
      "source": [
        "#### จบ Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kyrqLYF2YjO"
      },
      "source": [
        "### ความผิดปกติ 2) Semantic Anomalies\n",
        "\n",
        "**Integrity constraints**: ค่าอยู่นอกเหนือขอบเขตของค่าที่รับได้ เช่น\n",
        "- user_id: ค่าจะต้องเป็นตัวเลขหรือตัวหนังสือ 8 ตัวอักษร"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iasmydx2TEYG"
      },
      "source": [
        "# ดูว่าข้อมูล user_id ตอนนี้หน้าตาเป็นอย่างไร\n",
        "dt_clean.select(\"user_id\").show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVtj_wvpsdn4"
      },
      "source": [
        "# นับจำนวน user_id ทั้งหมด\n",
        "dt_clean.select(\"user_id\").count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzMCAOg5WlyX"
      },
      "source": [
        "#### Exercise 5\n",
        "\n",
        "หาว่า user_id ตรงตามรูปแบบที่เราต้องการมั้ย และแทนที่ด้วยค่าที่ใกล้เคียงถ้าไม่ตรง"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaViXptRXJXQ"
      },
      "source": [
        "ดูว่า user_id ตรงตามรูปแบบที่เราต้องการ มีกี่แถว\n",
        "\n",
        "คำใบ้: ใช้เว็บไซต์ https://www.regex101.com เพื่อสร้าง Regular Expression ตามรูปแบบที่เราต้องการ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3H6sWVsrSn-"
      },
      "source": [
        "# แทนที่ ... ด้วย Regular Expression ของรูปแบบ user_id ที่เราต้องการ\n",
        "# คำใบ้: ใน Regular Expression ที่เราต้องการ มี ^ นำหน้า และลงท้ายด้วย $\n",
        "dt_clean.where(dt_clean[\"user_id\"].rlike(\"...\")).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6zGICZpXqlu"
      },
      "source": [
        "มาลองดูข้อมูลที่ไม่ถูกต้องบ้าง ว่าหน้าตาเป็นแบบไหน"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LrPP5bgGFIi"
      },
      "source": [
        "![](https://file.designil.com/MmVhZf+)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tE-VcSQXWzp"
      },
      "source": [
        "# คำเตือน: Cell นี้อาจจะใช้เวลาประมาณ 15 วินาที\n",
        "\n",
        "# แทนที่ ... ด้วย Regular Expression ของรูปแบบ user_id ที่เราต้องการ\n",
        "dt_correct_userid = dt_clean.filter(dt_clean[\"user_id\"].rlike(\"...\"))\n",
        "dt_incorrect_userid = dt_clean.subtract(dt_correct_userid)\n",
        "\n",
        "dt_incorrect_userid.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ekkpo7XyDG"
      },
      "source": [
        "มาทำการแก้ไข user_id นี้ให้ถูกต้องกันเถอะ (ตัวที่เป็น null ยังไม่ต้องแก้ไข)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2SAQYLuuKgX"
      },
      "source": [
        "# แทนค่าที่ผิด ด้วยค่าที่ถูกต้อง (โค้ดจาก Exercise 4)\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ko7m6mbwJJj"
      },
      "source": [
        "# ตรวจสอบผลลัพธ์\n",
        "dt_correct_userid = dt_clean_userid.filter(dt_clean_userid[\"user_id\"].rlike(\"...\"))\n",
        "dt_incorrect_userid = dt_clean_userid.subtract(dt_correct_userid)\n",
        "\n",
        "dt_incorrect_userid.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6LoL8etxNSO"
      },
      "source": [
        "# เอาคอลัมน์ user_id_update ไปแทนที่ user_id (โค้ดจาก Exercise 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLS42g75bgOR"
      },
      "source": [
        "#### จบ Exercise 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kCf3LsX2b8L"
      },
      "source": [
        "### ความผิดปกติ 3) Missing values\n",
        "\n",
        "การเช็คและแก้ไข Missing Values (หากจำเป็น)\n",
        "\n",
        "ค่า Missing Value คือ ค่าที่ว่างเปล่า\n",
        "\n",
        "เราจะรู้ได้ยังไงว่าคอลัมน์ไหนมีค่าว่างเปล่ากี่ค่า"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T11c7iUs24Cu"
      },
      "source": [
        "# วิธีที่ 1 ในการเช็ค Missing Value\n",
        "# ใช้เทคนิค List Comparehension - ทบทวนได้ใน Pre-course Python https://school.datath.com/courses/road-to-data-engineer-2/contents/6129b780564a8\n",
        "# เช่น [ print(i) for i in [1,2,3] ]\n",
        "\n",
        "# col = คำสั่ง Spark ในการเลือกคอลัมน์\n",
        "# sum = คำสั่ง Spark ในการคิดผลรวม\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "dt_nulllist = dt_clean.select([ sum(col(colname).isNull().cast(\"int\")).alias(colname) for colname in dt_clean.columns ])\n",
        "dt_nulllist.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH7jfJTZdFwk"
      },
      "source": [
        "# วิธีที่ 2 ในการเช็ค Missing Value - จาก Exercise 1 โค้ดสะอาดกว่ามาก แต่ต้องมาบวกลบเอง\n",
        "dt_clean.summary(\"count\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bi3ql4PzXae"
      },
      "source": [
        "# ดูช้อมูลว่าแถวไหนมี user_id เป็นค่าว่างเปล่า (โค้ดเดียวกับ Exercise 1)\n",
        "\n",
        "dt_clean.where( dt_clean.user_id.isNull() ).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp3uwpf-Ty8Y"
      },
      "source": [
        "#### Exercise 6:\n",
        "ทางทีม Data Analyst แจ้งว่าอยากให้เราแทน user_id ที่เป็น NULL ด้วย 00000000 ไปเลย"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnwxPdTHTyS5"
      },
      "source": [
        "# Answer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJUNqs_1e1SY"
      },
      "source": [
        "# เช็คว่า user ID ที่เป็น NULL หายไปแล้วจริงมั้ย\n",
        "dt_clean.where( dt_clean.user_id.isNull() ).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9WmtPhUOJHi"
      },
      "source": [
        "### ความผิดปกติ 4) Outliers:\n",
        "\n",
        "ข้อมูลที่สูงหรือต่ำผิดปกติจากข้อมูลส่วนใหญ่\n",
        "\n",
        "มาลองใช้ Boxplot ในการหาค่า Outlier ของราคาหนังสือ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz_NYXoNfmBM"
      },
      "source": [
        "# Cell นี้จะรันค่อนข้างนาน เนื่องจากข้อมูลมีเยอะ\n",
        "dt_clean_pd = dt_clean.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejSTm4SvfOeX"
      },
      "source": [
        "sns.boxplot(x = dt_clean_pd['price'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI-MxExWfsXH"
      },
      "source": [
        "เห็นได้ว่ามีหนังสือบางเล่มที่ราคาสูงกว่าปกติไปเยอะมาก ลองมาดูกันว่าหนังสือ book_id อะไรบ้าง ที่ราคาเกิน $80"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv--yXibgjal"
      },
      "source": [
        "dt_clean.where( dt_clean.price > 80 ).select(\"book_id\").distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK7tOP_OgsOh"
      },
      "source": [
        "เราสามารถนำ Book_ID อันนี้ไปเช็คต่อกับแหล่งข้อมูลได้ ว่าเป็นหนังสืออะไร และราคาเกิน $80 ผิดปกติมั้ย\n",
        "\n",
        "ถ้าเอาไปเช็คในข้อมูลจาก Workshop 1 ก็จะพบว่า Book_ID = 635 คือ หนังสือชื่อ \"The Power Broker\"\n",
        "https://www.audible.com/pd/The-Power-Broker-Audiobook/B0051JH67K?ipRedirectOverride=true&overrideBaseCountry=true&pf_rd_p=2756bc30-e1e4-4174-bb22-bce00b971761&pf_rd_r=MF7KC1JQF3A6GK2ET8XM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt7qEffui33t"
      },
      "source": [
        "![](https://file.designil.com/7h1WIp+)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJij1Esri-r0"
      },
      "source": [
        "The Power Broker มีราคา $84 จริง และเป็นหนังสือเสียงที่มีความยาวถึง 66 ชั่วโมง\n",
        "\n",
        "**ในที่นี้ ถือว่าเป็น Outlier จริง แต่ไม่ได้เป็นข้อมูลที่ผิด จึงไม่ต้องแก้อะไร**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5DtVcoRUsFp"
      },
      "source": [
        "### มาลอง Clean ข้อมูลด้วย Spark SQL\n",
        "\n",
        "![alt text](https://cdn-std.droplr.net/files/acc_513973/881iHw)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMwaNBsSuygq"
      },
      "source": [
        "# แปลงข้อมูลจาก Spark DataFrame ให้เป็น TempView ก่อน\n",
        "dt.createOrReplaceTempView(\"data\")\n",
        "dt_sql = spark.sql(\"SELECT * FROM data\")\n",
        "dt_sql.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQLzG7jz06vU"
      },
      "source": [
        "# ลองแปลงโค้ดสำหรับลิสต์ชื่อประเทศ Exercise 4 เป็น SQL\n",
        "dt_sql_country = spark.sql(\"\"\"\n",
        "SELECT distinct country\n",
        "FROM data\n",
        "ORDER BY country\n",
        "\"\"\")\n",
        "dt_sql_country.show(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFnsr05vYnOi"
      },
      "source": [
        "# ลองแปลงโค้ดสำหรับแทนที่ชื่อประเทศ จาก Exercise 4 เป็น SQL\n",
        "dt_sql_result = spark.sql(\"\"\"\n",
        "SELECT timestamp, user_id, book_id,\n",
        "  CASE WHEN country = 'Japane' THEN 'Japan' ELSE country END AS country,\n",
        "price\n",
        "FROM data\n",
        "\"\"\")\n",
        "dt_sql_result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4dAJncSmK1g"
      },
      "source": [
        "# เช็คผลลัพธ์ว่าถูกจริงมั้ย\n",
        "dt_sql_result.select(\"country\").distinct().sort(\"country\").show(58, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv0YK6cFgSiB"
      },
      "source": [
        "#### Exercise 7\n",
        "\n",
        "ทำ Exercise 5 ด้วย SQL\n",
        "\n",
        "คำใบ้: ใช้คำสั่ง RLIKE ใน SQL เพื่อตรวจเช็ครูปแบบ Regular Expression ได้"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QHlrKnPgMRh"
      },
      "source": [
        "# Answer here: เช็คว่ามีข้อมูล user_id ที่ไม่เป็นตัวหนังสือหรือตัวเลข 8 หลักมั้ย"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgCm2SiKgMP7"
      },
      "source": [
        "# Answer here: แทนค่า (คำใบ้: ใช้ CASE WHEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZUucKv1q8hB"
      },
      "source": [
        "# เช็คว่าข้อมูลที่ผิด หายไปหรือยัง\n",
        "dt_sql_uid_result.where( dt_sql_uid_result.user_id == 'ca86d17200' ).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ7-Up-N2N_B"
      },
      "source": [
        "---\n",
        "\n",
        "![](https://file.designil.com/TmpQfK+)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu3WQ4g8UBra"
      },
      "source": [
        "# Step 5) Save data เป็น CSV\n",
        "\n",
        "โดยปกติแล้ว Spark จะทำการ Save ออกมาเป็นหลายไฟล์ เพราะใช้หลายเครื่องในการประมวลผล"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6ZMEZ-8UIEJ"
      },
      "source": [
        "# เซฟเป็น partitioned files (ใช้ multiple workers)\n",
        "dt_clean.write.csv('Cleaned_data.csv', header = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcPc7I-BYckx"
      },
      "source": [
        "เราสามารถบังคับให้ Spark เซฟมาเป็นไฟล์เดียวได้"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzMterYlWA5X"
      },
      "source": [
        "# เซฟเป็น 1 ไฟล์ (ใช้ single worker)\n",
        "dt_clean.coalesce(1).write.csv('Cleaned_Data_Single.csv', header = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHJ93iwFY_rS"
      },
      "source": [
        "ยังไม่จบแค่นี้ เรามีแถมอีกเรื่อง...\n",
        "\n",
        "### Bonus: วิธีอ่านไฟล์ที่มีหลาย Part\n",
        "เช่น กรณีนี้ที่เรามี\n",
        "- /content/Cleaned_Data.csv/part-00000-....csv\n",
        "- /content/Cleaned_Data.csv/part-00001-....csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJN1WjglaKK4"
      },
      "source": [
        "all_parts = spark.read.csv('/content/Cleaned_data.csv/part-*.csv', header = True, inferSchema = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxgNMXk_Ss_3"
      },
      "source": [
        "all_parts.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBpcOjJLr95Q"
      },
      "source": [
        "print('จบ Workshop 2 แล้วคร้าบ 😍')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}